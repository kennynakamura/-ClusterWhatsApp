{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WhatsAppCluster.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMzhFJY/AI+FXIeBo+03yk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kennynakamura/ClusterConversaWhatsApp/blob/main/WhatsAppCluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzHtZD3W5TqN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114a6756-3abd-4556-b5a2-d49aa5a6028b"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import plotly\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import string\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE1wqh7qKadM"
      },
      "source": [
        "def starts_with_date_time(s):\n",
        "    pattern = '^([0-9]+)(\\/)([0-9]+)(\\/)([0-9]+)[ ]([0-9]+):([0-9]+)[ ]-'\n",
        "    result = re.match(pattern, s)\n",
        "    if result:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def find_author(s):\n",
        "  s=s.split(\":\")\n",
        "  if len(s) > 1:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def split_count(text):\n",
        "    emoji_list = []\n",
        "    data = regex.findall(r'\\X', text)\n",
        "    for word in data:\n",
        "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
        "            emoji_list.append(word)\n",
        "    return emoji_list\n",
        "\n",
        "def get_data_point(line):\n",
        "    split_l = line.split(' - ')\n",
        "    date_time = split_l[0]\n",
        "    date, time = date_time.split(' ')\n",
        "    message = ' '.join(split_l[1:])\n",
        "    split_msg = message.split(': ')\n",
        "    author = split_msg[0]\n",
        "    message = ' '.join(split_msg[1:])\n",
        "    return date, time, author, message\n",
        "\n",
        "def tiraroculto(dataframe):\n",
        "  dataframe = dataframe.drop(df[df.message =='<Arquivo de mídia oculto>'].index)\n",
        "  return dataframe\n",
        "\n",
        "def Textos(path):\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith('.txt'):\n",
        "           with open(os.path.join(path, filename)) as f:\n",
        "                nome = Path(filename).name\n",
        "                titulo.append(nome)\n",
        "                linha = f.read()\n",
        "                lista.append(linha)\n",
        "    return\n",
        "\n",
        "def Grafico(a,b):\n",
        "  K = range(a,b)\n",
        "  for k in K:\n",
        "      km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n",
        "      km = km.fit(X)\n",
        "      Sum_of_squared_distances.append(km.inertia_)\n",
        "  plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "  plt.xlabel('k')\n",
        "  plt.ylabel('Sum_of_squared_distances')\n",
        "  plt.title('Elbow Method For Optimal k')\n",
        "  plt.show()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-QRYSJgNyt6"
      },
      "source": [
        "k = NUMERO DE CLUSTERS\n",
        "\n",
        "\n",
        "\n",
        "ll = []\n",
        "#Colocar o endereço da pasta \"data\"\n",
        "path = \"/content/data/\"\n",
        "for filename in os.listdir(path):\n",
        "   parsed = []\n",
        "   nome = Path(path + filename).name\n",
        "   if nome.endswith(\".txt\"):\n",
        "     with open(path + nome, encoding=\"utf-8\") as fp:\n",
        "       fp.readline()\n",
        "       name = Path(nome).stem\n",
        "       msg_buffer = []\n",
        "       date, time, author = None, None, None\n",
        "       while True:\n",
        "          line = fp.readline()\n",
        "          if not line:\n",
        "              break\n",
        "          line = line.strip()\n",
        "          if starts_with_date_time(line):\n",
        "              if len(msg_buffer) > 0:\n",
        "                  parsed.append([date, time, author, ' '.join(msg_buffer), name])\n",
        "              msg_buffer.clear()\n",
        "              date, time, author, message = get_data_point(line)\n",
        "              msg_buffer.append(message)\n",
        "          else:\n",
        "              msg_buffer.append(line)\n",
        "     nome = nome.replace(\" \",\"\")\n",
        "     df = pd.DataFrame(parsed, columns=['date', 'time', 'author', 'message', 'name'])\n",
        "     df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "     df = tiraroculto(df)\n",
        "     df = df['message']\n",
        "     ll.append((str(df)))\n",
        "\n",
        "lista = []\n",
        "titulo = []\n",
        "Sum_of_squared_distances = []\n",
        "#Endereço para acessar a pasta \"data\". obs: sem o '/'\n",
        "Textos('/content/data')\n",
        "vectorizer = TfidfVectorizer(stop_words={'portuguese'})\n",
        "X = vectorizer.fit_transform(ll)\n",
        "Grafico(2,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl9Ic_DWHSg9",
        "outputId": "e05cdb44-1588-4040-b5be-e01bbfe0472b"
      },
      "source": [
        "true_k = k\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\n",
        "model.fit(X)\n",
        "labels=model.labels_\n",
        "tabela=pd.DataFrame(list(zip(titulo,labels)),columns=['Título','cluster'])\n",
        "print(tabela.sort_values(by=['cluster']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                           Título  cluster\n",
            "1  Conversa do WhatsApp com Papelaria Yoshida.txt        0\n",
            "0      Conversa do WhatsApp com Marins ML (1).txt        1\n",
            "3            Conversa do WhatsApp com Claudio.txt        1\n",
            "2          Conversa do WhatsApp com Danilo IF.txt        2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}